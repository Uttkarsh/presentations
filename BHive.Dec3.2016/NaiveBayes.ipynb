{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, math, collections, itertools, os\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.metrics import precision\n",
    "from nltk.metrics import recall\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source code for NLTK Naive Bayes is at http://www.nltk.org/_modules/nltk/classify/naivebayes.html\n",
    "\n",
    "First, load up the dataset. This data is for movie ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "POLARITY_DATA_DIR = os.path.join('polarityData', 'rt-polaritydata')\n",
    "RT_POLARITY_POS_FILE = os.path.join(POLARITY_DATA_DIR, 'rt-polarity-pos.txt')\n",
    "RT_POLARITY_NEG_FILE = os.path.join(POLARITY_DATA_DIR, 'rt-polarity-neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_features(feature_select):\n",
    "        posExamples = []\n",
    "        negExamples = []\n",
    "        \n",
    "        # http://stackoverflow.com/questions/367155/splitting-a-string-into-words-and-punctuation\n",
    "        # This breaks up the sentences into lists of individual words (as selected by the input mechanism) and \n",
    "        # appends 'pos' or 'neg' after each list\n",
    "        \n",
    "        with open(RT_POLARITY_POS_FILE, 'r') as posSentences:\n",
    "                for i in posSentences:\n",
    "                        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "                        posWords = [feature_select(posWords), 'pos']\n",
    "                        posExamples.append(posWords)\n",
    "        with open(RT_POLARITY_NEG_FILE, 'r') as negSentences:\n",
    "                for i in negSentences:\n",
    "                        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "                        negWords = [feature_select(negWords), 'neg']\n",
    "                        negExamples.append(negWords)\n",
    "\n",
    "        \n",
    "        # Randomly shuffle the positive and negative examples - this is often a good idea.\n",
    "        random.shuffle(posExamples)\n",
    "        random.shuffle(negExamples)\n",
    "        \n",
    "        # Selects trainingPercent of the features to be used for training and rest to be used for testing\n",
    "        # Typical breakups are 70% for training, 30% for testing. We can also do 75% and 25%.\n",
    "        trainingPercent = 0.70\n",
    "        posCutoff = int(math.floor(len(posExamples)*trainingPercent))\n",
    "        negCutoff = int(math.floor(len(negExamples)*trainingPercent))\n",
    "        trainExamples = posExamples[:posCutoff] + negExamples[:negCutoff]\n",
    "        testExamples = posExamples[posCutoff:] + negExamples[negCutoff:]\n",
    "\n",
    "        # Trains a Naive Bayes Classifier\n",
    "        classifier = NaiveBayesClassifier.train(trainExamples)\n",
    "\n",
    "        # Initiates referenceSets and testSets\n",
    "        referenceSets = collections.defaultdict(set)\n",
    "        testSets = collections.defaultdict(set)\n",
    "\n",
    "        #puts correctly labeled sentences in referenceSets and the predictively labeled version in testsets\n",
    "        for i, (features, label) in enumerate(testExamples):\n",
    "                referenceSets[label].add(i)\n",
    "                predicted = classifier.classify(features)\n",
    "                testSets[predicted].add(i)\n",
    "                \n",
    "        #prints metrics to show how well the feature selection did\n",
    "        print 'train on %d instances, test on %d instances' % (len(trainExamples), len(testExamples))\n",
    "        print 'accuracy: %.2f' %(100*nltk.classify.util.accuracy(classifier, testExamples) )\n",
    "        print 'pos precision: %.2f' % (100*precision(referenceSets['pos'], testSets['pos']))\n",
    "        print 'pos recall: %.2f' %(100*recall(referenceSets['pos'], testSets['pos']) )\n",
    "        print 'neg precision: %.2f'%(100*precision(referenceSets['neg'], testSets['neg']))\n",
    "        print 'neg recall: %.2f'%(100*recall(referenceSets['neg'], testSets['neg']))\n",
    "        classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all words as features\n",
      "train on 7462 instances, test on 3200 instances\n",
      "accuracy: 76.53\n",
      "pos precision: 77.02\n",
      "pos recall: 75.62\n",
      "neg precision: 76.06\n",
      "neg recall: 77.44\n",
      "Most Informative Features\n",
      "                    flat = True              neg : pos    =     21.0 : 1.0\n",
      "                    warm = True              pos : neg    =     16.3 : 1.0\n",
      "              delightful = True              pos : neg    =     13.0 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.0 : 1.0\n",
      "                    dull = True              neg : pos    =     12.3 : 1.0\n",
      "                  stupid = True              neg : pos    =     11.7 : 1.0\n",
      "               inventive = True              pos : neg    =     11.7 : 1.0\n",
      "              refreshing = True              pos : neg    =     11.0 : 1.0\n",
      "               affecting = True              pos : neg    =     11.0 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#creates a feature selection mechanism that uses all words\n",
    "def make_full_dict(words):\n",
    "        return dict([(word, True) for word in words])\n",
    "    \n",
    "print 'using all words as features'\n",
    "evaluate_features(make_full_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means: the classifier found the word \"wonderful\" to be 15.7 times more indicative of a positive review as against it being a negative review. For more details, look at http://www.nltk.org/book/ch06.html#document-classify-smif. The NLTK code is at http://www.nltk.org/_modules/nltk/classify/naivebayes.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_word_scores():\n",
    "        #creates lists of all positive and negative words\n",
    "        posWords = []\n",
    "        negWords = []\n",
    "        with open(RT_POLARITY_POS_FILE, 'r') as posSentences:\n",
    "                for i in posSentences:\n",
    "                        posWord = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "                        posWords.append(posWord)\n",
    "        with open(RT_POLARITY_NEG_FILE, 'r') as negSentences:\n",
    "                for i in negSentences:\n",
    "                        negWord = re.findall(r\"[\\w']+|[.,!?;]\", i.rstrip())\n",
    "                        negWords.append(negWord)\n",
    "        posWords = list(itertools.chain(*posWords))\n",
    "        negWords = list(itertools.chain(*negWords))\n",
    "\n",
    "        #build frequency distibution of all words and then frequency distributions of words within positive and negative labels\n",
    "        word_fd = FreqDist()\n",
    "        cond_word_fd = ConditionalFreqDist()\n",
    "        for word in posWords:\n",
    "                word_fd[word.lower()] += 1\n",
    "                cond_word_fd['pos'][word.lower()] += 1\n",
    "        for word in negWords:\n",
    "                word_fd[word.lower()] += 1\n",
    "                cond_word_fd['neg'][word.lower()] += 1\n",
    "\n",
    "        #finds the number of positive and negative words, as well as the total number of words\n",
    "        pos_word_count = cond_word_fd['pos'].N()\n",
    "        neg_word_count = cond_word_fd['neg'].N()\n",
    "        total_word_count = pos_word_count + neg_word_count\n",
    "\n",
    "        #builds dictionary of word scores based on chi-squared test\n",
    "        word_scores = {}\n",
    "        for word, freq in word_fd.iteritems():\n",
    "                pos_score = BigramAssocMeasures.chi_sq(cond_word_fd['pos'][word], (freq, pos_word_count), total_word_count)\n",
    "                neg_score = BigramAssocMeasures.chi_sq(cond_word_fd['neg'][word], (freq, neg_word_count), total_word_count)\n",
    "                word_scores[word] = pos_score + neg_score\n",
    "\n",
    "        return word_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating best 10 word features\n",
      "train on 7462 instances, test on 3200 instances\n",
      "accuracy: 56.66\n",
      "pos precision: 54.36\n",
      "pos recall: 83.06\n",
      "neg precision: 64.11\n",
      "neg recall: 30.25\n",
      "Most Informative Features\n",
      "                    dull = True              neg : pos    =     12.8 : 1.0\n",
      "                  boring = True              neg : pos    =     10.1 : 1.0\n",
      "                     bad = True              neg : pos    =      5.3 : 1.0\n",
      "                     too = True              neg : pos    =      3.6 : 1.0\n",
      "                       ? = True              neg : pos    =      2.2 : 1.0\n",
      "                      no = True              neg : pos    =      2.2 : 1.0\n",
      "                    just = True              neg : pos    =      1.9 : 1.0\n",
      "                   movie = True              neg : pos    =      1.6 : 1.0\n",
      "                     and = True              pos : neg    =      1.2 : 1.0\n",
      "                     and = None              neg : pos    =      1.2 : 1.0\n",
      "evaluating best 100 word features\n",
      "train on 7462 instances, test on 3200 instances\n",
      "accuracy: 68.22\n",
      "pos precision: 66.50\n",
      "pos recall: 73.44\n",
      "neg precision: 70.34\n",
      "neg recall: 63.00\n",
      "Most Informative Features\n",
      "                   worst = True              neg : pos    =     22.3 : 1.0\n",
      "               wonderful = True              pos : neg    =     17.0 : 1.0\n",
      "                  boring = True              neg : pos    =     16.2 : 1.0\n",
      "                    warm = True              pos : neg    =     15.0 : 1.0\n",
      "                mediocre = True              neg : pos    =     14.3 : 1.0\n",
      "                 generic = True              neg : pos    =     13.0 : 1.0\n",
      "                provides = True              pos : neg    =     12.3 : 1.0\n",
      "                   fails = True              neg : pos    =     11.8 : 1.0\n",
      "                    flat = True              neg : pos    =     11.0 : 1.0\n",
      "                  stupid = True              neg : pos    =     10.2 : 1.0\n",
      "evaluating best 1000 word features\n",
      "train on 7462 instances, test on 3200 instances\n",
      "accuracy: 79.78\n",
      "pos precision: 80.37\n",
      "pos recall: 78.81\n",
      "neg precision: 79.22\n",
      "neg recall: 80.75\n",
      "Most Informative Features\n",
      "             pretentious = True              neg : pos    =     17.7 : 1.0\n",
      "               wonderful = True              pos : neg    =     17.0 : 1.0\n",
      "              engrossing = True              pos : neg    =     15.0 : 1.0\n",
      "              unexpected = True              pos : neg    =     14.3 : 1.0\n",
      "                   waste = True              neg : pos    =     13.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "                    dull = True              neg : pos    =     12.6 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "              delightful = True              pos : neg    =     12.3 : 1.0\n",
      "                 routine = True              neg : pos    =     12.3 : 1.0\n",
      "evaluating best 10000 word features\n",
      "train on 7462 instances, test on 3200 instances\n",
      "accuracy: 84.09\n",
      "pos precision: 84.81\n",
      "pos recall: 83.06\n",
      "neg precision: 83.40\n",
      "neg recall: 85.12\n",
      "Most Informative Features\n",
      "                provides = True              pos : neg    =     15.0 : 1.0\n",
      "                 numbers = True              neg : pos    =     14.3 : 1.0\n",
      "                  stupid = True              neg : pos    =     13.0 : 1.0\n",
      "               inventive = True              pos : neg    =     13.0 : 1.0\n",
      "              engrossing = True              pos : neg    =     13.0 : 1.0\n",
      "                mediocre = True              neg : pos    =     12.3 : 1.0\n",
      "             examination = True              pos : neg    =     11.7 : 1.0\n",
      "                   waste = True              neg : pos    =     11.0 : 1.0\n",
      "                  flawed = True              pos : neg    =     11.0 : 1.0\n",
      "                    flat = True              neg : pos    =     10.6 : 1.0\n",
      "evaluating best 15000 word features\n",
      "train on 7462 instances, test on 3200 instances\n",
      "accuracy: 85.00\n",
      "pos precision: 85.53\n",
      "pos recall: 84.25\n",
      "neg precision: 84.48\n",
      "neg recall: 85.75\n",
      "Most Informative Features\n",
      "                  boring = True              neg : pos    =     24.3 : 1.0\n",
      "              engrossing = True              pos : neg    =     15.7 : 1.0\n",
      "                captures = True              pos : neg    =     15.7 : 1.0\n",
      "               wonderful = True              pos : neg    =     15.0 : 1.0\n",
      "                    warm = True              pos : neg    =     14.3 : 1.0\n",
      "                powerful = True              pos : neg    =     13.4 : 1.0\n",
      "                  unique = True              pos : neg    =     12.3 : 1.0\n",
      "                provides = True              pos : neg    =     12.3 : 1.0\n",
      "                  stupid = True              neg : pos    =     12.3 : 1.0\n",
      "                  flawed = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Finds word scores\n",
    "word_scores = create_word_scores()\n",
    "\n",
    "# Finds the best 'number' words based on word scores\n",
    "def find_best_words(word_scores, number):\n",
    "        best_vals = sorted(word_scores.iteritems(), key=lambda (w, s): s, reverse=True)[:number]\n",
    "        best_words = set([w for w, s in best_vals])\n",
    "        return best_words\n",
    "\n",
    "# Creates feature selection mechanism that only uses best words\n",
    "def best_word_features(words):\n",
    "        return dict([(word, True) for word in words if word in best_words])\n",
    "\n",
    "# Numbers of features to select\n",
    "numbers_to_test = [10, 100, 1000, 10000, 15000]\n",
    "# Tries the best_word_features mechanism with each of the numbers_to_test of features\n",
    "for num in numbers_to_test:\n",
    "        print 'evaluating best %d word features' % (num)\n",
    "        best_words = find_best_words(word_scores, num)\n",
    "        evaluate_features(best_word_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
